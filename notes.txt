objectMapper.registerModule(new EventMessageModule());

public class EventMessageModule extends SimpleModule {

    private static final String NAME = "EventMessageModule";

    public EventMessageModule() {
        super(NAME);
        addSerializer(EventMessage.class, new EventMessageSerializer());
        addDeserializer(EventMessage.class, new EventMessageDeserializer());
    }
}

public class EventMessageSerializer extends JsonSerializer<EventMessage> {

    public static final String EVENT_METADATA_KEY = "metadata";
    public static final String EVENT_DATA_KEY = "data";

    @Override
    public void serialize(EventMessage value, JsonGenerator gen, SerializerProvider serializers) throws IOException {
        gen.writeStartObject();
        gen.writeObjectField(EVENT_METADATA_KEY, value.getMetadata());
        System.out.println(value.getEvent().toString());
        gen.writeObjectField(EVENT_DATA_KEY, value.getEvent());

        gen.writeEndObject();
    }
}

public class EventMessageDeserializer extends JsonDeserializer<EventMessage> {

    private static final RuntimeAggregateEventClassCache eventClassCache = RuntimeAggregateEventClassCache.getInstance();

    @Override
    public EventMessage deserialize(JsonParser jp, DeserializationContext ctxt)
        throws IOException {
        ObjectMapper objectMapper = (ObjectMapper) jp.getCodec();
        JsonNode node = jp.getCodec().readTree(jp);
        TreeNode metadata = node.get(EVENT_METADATA_KEY);
        if (metadata == null) {
            throw new IllegalStateException("metadata object required to be deserialized");
        }

        JsonNode type = (JsonNode) metadata.get("type");
        if (type == null) {
            throw new IllegalStateException("event type object required to be deserialized");
        }

        String typeValue = type.textValue();
        Optional<Class<? extends Event>> maybeClzz = eventClassCache.get(typeValue);
        Class<? extends Event> clzz = maybeClzz.orElseThrow(() -> new AggregateEventClassNotFoundException(typeValue));
//
        JsonParser eventNode = node.get(EVENT_DATA_KEY).traverse();
        Event event = (Event) objectMapper.readValue(eventNode, clzz);
        EventMetadata eventMetadata = objectMapper.readValue(metadata.traverse(), EventMetadata.class);
//
        return EventMessage.newBuilder()
            .event(event)
            .metadata(eventMetadata)
            .build();
    }
}

@Singleton
@KafkaListener(groupId = "${kafka.event.group-id}")
public class KafkaEventProcessor implements ConsumerRebalanceListener, ConsumerAware<String, byte[]>,
    KafkaListenerExceptionHandler {

    private final ObjectMapper objectMapper;
    private Consumer<String, byte[]> consumer;

    private final KafkaDeadLetterEventPublisher deadLetterEventPublisher;
    private final KafkaPoisonEventPublisher poisonEventPublisher;

    public KafkaEventProcessor(ObjectMapper objectMapper,
        KafkaDeadLetterEventPublisher deadLetterEventPublisher,
        KafkaPoisonEventPublisher poisonEventPublisher) {
        this.objectMapper = objectMapper;
        this.deadLetterEventPublisher = deadLetterEventPublisher;
        this.poisonEventPublisher = poisonEventPublisher;
    }

    @Topic("${kafka.event.topic}")
    public void receive(ConsumerRecord<String, byte[]> records) throws Exception {
        byte[] value = records.value();
        String key = records.key();
        System.out.println("Key: " + key + ", value: " + new String(value));
        int keyId = Integer.parseInt(key);
        if (keyId > 1) {
            throw new PoisonException(value, "Key Id unexpected: " + keyId);
        }

        consumer.commitSync(Collections.singletonMap(
            new TopicPartition(records.topic(), records.partition()),
            new OffsetAndMetadata(records.offset() + 1, "my metadata")
        ));

//        }
    }

    @Topic("${kafka.event.poison.topic}")
    public void receivePoisonMessage(ConsumerRecord<String, byte[]> record) {
        Header retryCount = record.headers().lastHeader("retryCount");
//        if(retryCount != null) {
//            retryCount.
//        }
    }

    @Override
    public void setKafkaConsumer(@NotNull Consumer<String, byte[]> consumer) {
        this.consumer = consumer;
    }

    @Override
    public void onPartitionsRevoked(Collection<TopicPartition> partitions) {

    }

    @Override
    public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
// seek to offset here
        for (TopicPartition partition : partitions) {
            consumer.seek(partition, 1);
        }
    }

    @Override
    public void handle(KafkaListenerException exception) {
        Optional<ConsumerRecord<?, ?>> consumerRecord = exception.getConsumerRecord();
//        Throwable cause = exception.getCause();
        Throwable causeUsingPlainJava = findCauseUsingPlainJava(exception);
        if (PoisonException.class.isAssignableFrom(causeUsingPlainJava.getClass())) {
            System.out.println("Poison Cause: " + causeUsingPlainJava.getMessage());
        }
        if (consumerRecord.isPresent()) {
            @SuppressWarnings("unchecked")
            ConsumerRecord<String, byte[]> consumerRecord1 = (ConsumerRecord<String, byte[]>) consumerRecord.get();
            byte[] value = consumerRecord1.value();
            System.out.println("Exception caught record [Key: " + consumerRecord1.key() + ", Record: "
                + new String(value) + "]");
            System.out.println("Got Exception -> " + exception.getMessage());

//            deadLetterEventPublisher.publish(consumerRecord1.key(),
//                consumerRecord1.headers(), "");
//            try {
//                EventRecord eventRecord = deserialize(value);
//                deadLetterEventPublisher.publish(
//                    consumerRecord1.key(),
//                    eventRecord.metadata.partitionKey(),
//                    consumerRecord1.headers(),
//                    (Event) eventRecord.event
//                );
//            } catch (IOException | ClassNotFoundException e) {
//                throw new RuntimeException(e);
//            }

            exception.getKafkaConsumer()
                .commitSync(Collections.singletonMap(
                    new TopicPartition(consumerRecord1.topic(), consumerRecord1.partition()),
                    new OffsetAndMetadata(consumerRecord1.offset() + 1, "my metadata")
                ));
        } else {
            System.out.println("No Body to commit offset");
            System.out.println("Got Exception -> " + exception.getMessage());

        }


    }

    public static Throwable findCauseUsingPlainJava(Throwable throwable) {
        Objects.requireNonNull(throwable);
        Throwable rootCause = throwable;
        while (rootCause.getCause() != null && rootCause.getCause() != rootCause) {
            rootCause = rootCause.getCause();
        }
        return rootCause;
    }

    EventRecord deserialize(byte[] value) throws IOException, ClassNotFoundException {
        JsonNode jsonNode = objectMapper.readTree(value);
        String objectType = jsonNode.get("object_type").asText();
        if (objectType == null || objectType.isEmpty()) {
            return null;
        }
        Class<?> objectClass = Class.forName(objectType);
        String data = jsonNode.get("data").asText();
        if (data == null || data.isEmpty()) {
            return null;
        }

        String metadataObject = jsonNode.get("metadata").asText();
        if (metadataObject == null || metadataObject.isEmpty()) {
            return null;
        }

        Metadata metadata = objectMapper.readValue(metadataObject, Metadata.class);
        Object dataObject = objectMapper.readValue(data, objectClass);

        return new EventRecord(
            metadata, value, dataObject, objectClass, data
        );
    }

    // Headers headers = consumeRecord.headers();
    //				HashMap<String, Object> headerMap = new HashMap<>(8);
    //				Iterator<Header> iterator = headers.iterator();
    //				while (iterator.hasNext()) {
    //					Header next = iterator.next();
    //					headerMap.put(next.key(), serializer.deserialize(next.value()));
    //				}
}

--***---
package org.richard.event;

import io.micronaut.configuration.kafka.annotation.KafkaClient;
import io.micronaut.configuration.kafka.annotation.KafkaKey;
import io.micronaut.configuration.kafka.annotation.KafkaPartitionKey;
import io.micronaut.configuration.kafka.annotation.Topic;
import io.micronaut.context.annotation.Property;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.header.Headers;

@KafkaClient(
    acks = KafkaClient.Acknowledge.ALL,
    properties = @Property(name = ProducerConfig.RETRIES_CONFIG, value = "5")
)
public interface KafkaEventPublisher {

    @Topic("${kafka.producers.default.topic}")
    <T> RecordMetadata publish(
        @KafkaKey String messageKey,
        Headers headers,
        Event<T> event);

    @Topic("${kafka.producers.default.topic}")
    <T> RecordMetadata publish(
        @KafkaKey String messageKey,
        @KafkaPartitionKey String partitionKey,
        Headers headers,
        Event<T> event);


}

-- **
public class EventRecord {

    final Class<?> objectClass;
    byte[] source;
    Event event;
    Object dataObject;
    String eventData;
    Metadata metadata;

    public EventRecord(Metadata metadata, byte[] source, Object dataObject,
        Class<?> objectClass,
        String eventData){
        this.metadata = metadata;
        this.source = source;
        this.dataObject = dataObject;
        this.eventData = eventData;
        this.objectClass = objectClass;
    }
}

**
import java.time.Instant;

public record RetryPolicy(
    int maxRetry,
    int retryCounter,
    Instant nextRetry
) {}

--- **
@KafkaClient(
    id = "deadLetter",
    acks = KafkaClient.Acknowledge.ALL,
    properties = @Property(name = ProducerConfig.RETRIES_CONFIG, value = "5")
)
public interface KafkaDeadLetterEventPublisher {

    @Topic("${kafka.producers.deadLetter.topic}")
    <T> RecordMetadata publish(
        @KafkaKey String messageKey,
        Headers headers,
        Event<T> event);

    @Topic("${kafka.producers.deadLetter.topic}")
    <T> RecordMetadata publish(
        @KafkaKey String messageKey,
        @KafkaPartitionKey String partitionKey,
        Headers headers,
        Event<T> event);

}

--
public record Metadata(
    String correlationId,
    String partitionKey,
    RetryPolicy retry) {

}
